{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpgrulisC-tT"
      },
      "source": [
        "To run this, press \"*Runtime*\" and press \"*Run all*\" on a **free** Tesla T4 Google Colab instance!\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89KhZqVIC-tV"
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJzsvU1Pn_QK",
        "outputId": "582f3e99-4622-431d-9929-2114cf5040cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ§¹ Cleaning GPU memory...\n",
            "âœ… GPU memory cleared!\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import gc\n",
        "\n",
        "def clear_gpu_memory():\n",
        "    \"\"\"Clear all GPU memory immediately\"\"\"\n",
        "    print(\"ðŸ§¹ Cleaning GPU memory...\")\n",
        "\n",
        "    # Clear PyTorch cache\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "    # Force garbage collection\n",
        "    gc.collect()\n",
        "\n",
        "    print(\"âœ… GPU memory cleared!\")\n",
        "\n",
        "# Run this immediately\n",
        "clear_gpu_memory()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IKjoq_kkC-tV"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" huggingface_hub hf_transfer\n",
        "    # Install PyTorch 2.2.1\n",
        "    !pip install --no-deps unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y-bhMXxsyDQL"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install --no-deps git+https://github.com/huggingface/transformers.git # Need main branch for Liquid LFM2 models\n",
        "!pip install --no-deps causal-conv1d==1.5.0.post8 # Install Mamba kernels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YzhE4LDD89d"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHCIzjFQDnPP"
      },
      "outputs": [],
      "source": [
        "# pip install \"unsloth_zoo @ git+https://github.com/unslothai/unsloth-zoo.git\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_sHCD8amJuM",
        "outputId": "4cfe6e57-534e-45df-91c9-3e3476558ec1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Available devices: True, False\n",
            "Device count: CUDA=1, XPU=0\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(f\"Available devices: {torch.cuda.is_available()}, {torch.xpu.is_available()}\")\n",
        "print(f\"Device count: CUDA={torch.cuda.device_count()}, XPU={torch.xpu.device_count()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4uh63ke8mbyz",
        "outputId": "f54d5400-7a10-4add-ef53-c66c67f610b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using GPU: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import os\n",
        "\n",
        "# Force enable GPU detection\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # Try first GPU\n",
        "\n",
        "# Check if CUDA becomes available\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    print(\"No GPU available, falling back to CPU\")\n",
        "    device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6g6_GHNTjqJR"
      },
      "source": [
        "### Unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QmUBVEnvCDJv",
        "outputId": "481ede47-3240-4d44-90f8-eef51cdfdeed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth 2025.8.9: Fast Lfm2 patching. Transformers: 4.56.0.dev0.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.8.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.4.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = None. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.\n"
          ]
        }
      ],
      "source": [
        "from unsloth import FastModel\n",
        "import torch\n",
        "\n",
        "fourbit_models = [\n",
        "    # 4bit dynamic quants for superior accuracy and low memory use\n",
        "    \"unsloth/LFM2-1.2B-unsloth-bnb-4bit\",\n",
        "    \"unsloth/LFM2-700M-unsloth-bnb-4bit\",\n",
        "    \"unsloth/LFM2-350M-unsloth-bnb-4bit\",\n",
        "\n",
        "    # Full 16bit unquantized models\n",
        "    \"unsloth/LFM2-1.2B\",\n",
        "    \"unsloth/LFM2-700M\",\n",
        "    \"unsloth/LFM2-350M\",\n",
        "] # More models at https://huggingface.co/unsloth\n",
        "\n",
        "model, tokenizer = FastModel.from_pretrained(\n",
        "    model_name = \"unsloth/LFM2-1.2B\",\n",
        "    dtype = None, # None for auto detection\n",
        "    max_seq_length = 2048, # Choose any for long context!\n",
        "    load_in_4bit = False,  # 4 bit quantization to reduce memory\n",
        "    full_finetuning = False, # [NEW!] We have full finetuning now!\n",
        "    token = \"hf-token\", # use one if using gated models\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXd9bTZd1aaL"
      },
      "source": [
        "We now add LoRA adapters so we only need to update a small amount of parameters!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bZsfBuZDeCL",
        "outputId": "a13f0cb3-63ea-4939-bc3b-6a357cd1d936"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Making `model.base_model.model.model` require gradients\n",
            "Total parameters: 1,172,110,080\n",
            "Trainable parameters: 1,769,472\n",
            "Trainable percentage: 0.15%\n"
          ]
        }
      ],
      "source": [
        "model = FastModel.get_peft_model(\n",
        "    model,\n",
        "    finetune_vision_layers     = False, # LFM for now is just text only\n",
        "    finetune_language_layers   = True,  # Should leave on!\n",
        "    finetune_attention_modules = True,  # Attention good for GRPO\n",
        "    finetune_mlp_modules       = True,  # SHould leave on always!\n",
        "\n",
        "    r = 32,           # Larger = higher accuracy, but might overfit\n",
        "    lora_alpha = 16,  # Recommended alpha == r at least\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                  \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        ")\n",
        "# 3. Verify parameters are trainable\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Total parameters: {total_params:,}\")\n",
        "print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "print(f\"Trainable percentage: {100 * trainable_params / total_params:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vITh0KVJ10qX"
      },
      "source": [
        "<a name=\"Data\"></a>\n",
        "### Data Prep\n",
        "We now use the `LFM` format for conversation style finetunes. We use [Maxime Labonne's FineTome-100k](https://huggingface.co/datasets/mlabonne/FineTome-100k) dataset in ShareGPT style. LFM renders multi turn conversations like below:\n",
        "\n",
        "```\n",
        "<|startoftext|><|im_start|>user\n",
        "Hello!<|im_end|>\n",
        "<|im_start|>assistant\n",
        "Hey there!<|im_end|>\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "TigGJ1Ug7Ruo",
        "outputId": "2b8b011a-5d6a-4379-a7bd-c26b07008713"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'<|startoftext|><|im_start|>user\\nHello!<|im_end|>\\n<|im_start|>assistant\\nHey there!<|im_end|>\\n'"
            ]
          },
          "execution_count": 107,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.apply_chat_template([\n",
        "    {\"role\" : \"user\", \"content\" : \"Hello!\"},\n",
        "    {\"role\" : \"assistant\", \"content\" : \"Hey there!\"}\n",
        "], tokenize = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xmgrx_8QRahj",
        "outputId": "28223b43-b0ee-4782-cca5-87f316ce292d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (0.34.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (3.19.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (1.1.7)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (2025.8.3)\n"
          ]
        }
      ],
      "source": [
        "pip install huggingface_hub\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJTykn1L1EZ0",
        "outputId": "0e08279e-0c2f-44c3-f46b-9fae3f66626c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using the latest cached version of the dataset since bitext/Bitext-retail-ecommerce-llm-chatbot-training-dataset couldn't be found on the Hugging Face Hub\n",
            "WARNING:datasets.load:Using the latest cached version of the dataset since bitext/Bitext-retail-ecommerce-llm-chatbot-training-dataset couldn't be found on the Hugging Face Hub\n",
            "Found the latest cached dataset configuration 'default' at /root/.cache/huggingface/datasets/bitext___bitext-retail-ecommerce-llm-chatbot-training-dataset/default/0.0.0/12dd624ddcd3057382b2faad661bcda1fa869491 (last modified on Thu Aug 28 09:34:13 2025).\n",
            "WARNING:datasets.packaged_modules.cache.cache:Found the latest cached dataset configuration 'default' at /root/.cache/huggingface/datasets/bitext___bitext-retail-ecommerce-llm-chatbot-training-dataset/default/0.0.0/12dd624ddcd3057382b2faad661bcda1fa869491 (last modified on Thu Aug 28 09:34:13 2025).\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load the dataset, providing the token for authentication\n",
        "dataset = load_dataset(\"bitext/Bitext-retail-ecommerce-llm-chatbot-training-dataset\", split=\"train\", token=\"hf-token\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JDSOSC8Z8jIl"
      },
      "outputs": [],
      "source": [
        "# from unsloth.chat_templates import standardize_data_formats\n",
        "# dataset = standardize_data_formats(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRcPOUCm8lH8"
      },
      "source": [
        "Let's see how row 100 looks like!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ObTymi7E8jq8"
      },
      "outputs": [],
      "source": [
        "# dataset[100]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmtPLDiF8nvN"
      },
      "source": [
        "We now have to apply the chat template for `LFM` onto the conversations, and save it to `text`. We also remove the BOS token otherwise we'll get double BOS tokens!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gESWeKB6WKcl"
      },
      "outputs": [],
      "source": [
        "def format_chat(examples):\n",
        "    formatted_texts = []\n",
        "    for instruction, response in zip(examples[\"instruction\"], examples[\"response\"]):\n",
        "        # Create the conversation structure\n",
        "        messages = [\n",
        "            {\"role\": \"user\", \"content\": instruction},\n",
        "            {\"role\": \"assistant\", \"content\": response}\n",
        "        ]\n",
        "        # Apply the chat template using the loaded tokenizer\n",
        "        formatted_text = tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize = False,\n",
        "            add_generation_prompt = False,\n",
        "        )\n",
        "        # Remove the BOS token if present\n",
        "        formatted_texts.append(formatted_text.removeprefix(tokenizer.bos_token))\n",
        "\n",
        "    return {\"text\": formatted_texts}\n",
        "\n",
        "dataset = dataset.map(format_chat, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "6uft-uj-A41p",
        "outputId": "754ba62a-d26f-4dbf-9944-61f4aa9b5058"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'<|im_start|>user\\nI got to add an item to the cart<|im_end|>\\n<|im_start|>assistant\\nI\\'ll get right on it! I\\'m here to assist you in adding an item to your cart. Here\\'s a step-by-step guide to help you out:\\n\\n1. Browse through our {{Product Catalog}} to find the item you want to add to your cart.\\n2. Once you\\'ve found the item, click on the \"{{Add to Cart}}\" button or select the desired quantity from the available options.\\n3. You\\'ll be redirected to your cart page, where you can review your selected items.\\n4. If you want to continue shopping, simply click on the \"{{Continue Shopping}}\" button, and you\\'ll be taken back to the catalog.\\n5. If you\\'re ready to proceed, click on the \"{{Checkout}}\" button to finalize your purchase.\\n\\nIf you encounter any difficulties during this process, please don\\'t hesitate to let me know. I\\'m here to help you with any questions or concerns you may have. Happy shopping!<|im_end|>\\n'"
            ]
          },
          "execution_count": 110,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset[0][\"text\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idAEIeSQ3xdS"
      },
      "source": [
        "<a name=\"Train\"></a>\n",
        "### Train the model\n",
        "Now let's use Huggingface TRL's `SFTTrainer`! More docs here: [TRL SFT docs](https://huggingface.co/docs/trl/sft_trainer). We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qqn8QwIBItyp"
      },
      "outputs": [],
      "source": [
        "from trl import SFTTrainer, SFTConfig\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=dataset,\n",
        "    args=SFTConfig(\n",
        "        dataset_text_field = \"text\",\n",
        "        per_device_train_batch_size=8,\n",
        "        gradient_accumulation_steps=8,\n",
        "        warmup_steps=5,\n",
        "        num_train_epochs=1,\n",
        "        max_steps=60,\n",
        "        learning_rate=5e-6,\n",
        "        logging_steps=1,\n",
        "        optim=\"adamw_torch\",  # Changed from adamw_8bit\n",
        "        weight_decay=0.01,\n",
        "        lr_scheduler_type=\"cosine\",\n",
        "        seed=3407,\n",
        "        report_to=\"wandb\",\n",
        "        output_dir=\"ecommerce-chatbot\",\n",
        "        fp16=True,  # Explicitly disable mixed precision\n",
        "        max_grad_norm=1.0,  # Gradient clipping\n",
        "    ),\n",
        "    formatting_func=format_chat,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lY6rubTOAHsN"
      },
      "source": [
        "We also use Unsloth's `train_on_completions` method to only train on the assistant outputs and ignore the loss on the user's inputs. This helps increase accuracy of finetunes!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KIjHG-2u-GqN"
      },
      "outputs": [],
      "source": [
        "from unsloth.chat_templates import train_on_responses_only\n",
        "trainer = train_on_responses_only(\n",
        "    trainer,\n",
        "    instruction_part = \"<|im_start|>user\\n\",\n",
        "    response_part = \"<|im_start|>assistant\\n\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPmWWLYxAa_4"
      },
      "source": [
        "Let's verify masking the instruction part is done! Let's print the 100th row again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "zDJcRw2OAcIb",
        "outputId": "a5bf1e86-b891-4d80-af6b-d7d88d3ecc2b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'<|startoftext|><|im_start|>user\\nI want assistance to add items to the carg<|im_end|>\\n<|im_start|>assistant\\nOf course! I\\'m here to help you add items to your cart. Let me guide you through the process:\\n\\n1. Visit our website and browse through our wide range of products.\\n2. When you find an item you want to add, click on the \"Add to Cart\" or \"Add to Bag\" button.\\n3. You can continue shopping and adding more items to your cart.\\n4. Once you\\'ve added all the items you want, click on the cart icon or the \"View Cart\" button to review your selections.\\n5. From there, you can update quantities, remove items, or proceed to checkout.\\n\\nIf you encounter any issues or have any questions along the way, feel free to reach out to our customer support team for further assistance. Happy shopping!<|im_end|>\\n'"
            ]
          },
          "execution_count": 130,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.decode(trainer.train_dataset[100][\"input_ids\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IeE-Ly0fAe17"
      },
      "source": [
        "Now let's print the masked out example - you should see only the answer is present:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "cB5Cq_3bAfKr",
        "outputId": "be2cb11c-84e0-45f5-c15c-c1e9c3a4d425"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'                  Of course! I\\'m here to help you add items to your cart. Let me guide you through the process:\\n\\n1. Visit our website and browse through our wide range of products.\\n2. When you find an item you want to add, click on the \"Add to Cart\" or \"Add to Bag\" button.\\n3. You can continue shopping and adding more items to your cart.\\n4. Once you\\'ve added all the items you want, click on the cart icon or the \"View Cart\" button to review your selections.\\n5. From there, you can update quantities, remove items, or proceed to checkout.\\n\\nIf you encounter any issues or have any questions along the way, feel free to reach out to our customer support team for further assistance. Happy shopping!<|im_end|>\\n'"
            ]
          },
          "execution_count": 131,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.decode([tokenizer.pad_token_id if x == -100 else x for x in trainer.train_dataset[100][\"labels\"]]).replace(tokenizer.pad_token, \" \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ejIt2xSNKKp",
        "outputId": "33b06af6-9f3e-4fe3-918b-1056f6285e05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU = Tesla T4. Max memory = 14.741 GB.\n",
            "10.334 GB of memory reserved.\n"
          ]
        }
      ],
      "source": [
        "# @title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yqxqAZ7KJ4oL",
        "outputId": "462153c8-fa3b-4138-b952-52d913216058"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 44,884 | Num Epochs = 1 | Total steps = 60\n",
            "O^O/ \\_/ \\    Batch size per device = 8 | Gradient accumulation steps = 8\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (8 x 8 x 1) = 64\n",
            " \"-____-\"     Trainable parameters = 1,769,472 of 1,172,110,080 (0.15% trained)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [60/60 06:43, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.789100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.792600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.779300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.816700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.773500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.780600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.770000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.767400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.783900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.764000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.795600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.782100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.776000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.793200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.758200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.766100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.765000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.797300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.788500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.812900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>0.790800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>0.736000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>0.766200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>0.768200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.762100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>0.804400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>0.781200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>0.801500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>0.782100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.757300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>0.780300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>0.772400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>0.768400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>0.781800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>0.797000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>0.765000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>0.753900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>0.739500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>0.772400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.788400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>0.762400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>0.796900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>0.786400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>0.773300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>0.749400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>0.777300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>0.768200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>0.790800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>0.804000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.755900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>0.765800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>0.796600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>0.782300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>0.827400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>0.770100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>0.829000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>0.789800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>0.825200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59</td>\n",
              "      <td>0.766900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.801500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/peft/utils/other.py:1228: UserWarning: Unable to fetch remote file due to the following error 401 Client Error: Unauthorized for url: https://huggingface.co/unsloth/LFM2-1.2B/resolve/main/config.json (Request ID: Root=1-68b02d7b-739abeed29b353620c0359fe;6ae21eb4-3d84-4804-8e7b-cf9a09985026)\n",
            "\n",
            "Invalid credentials in Authorization header - silently ignoring the lookup for the file config.json in unsloth/LFM2-1.2B.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/peft/utils/save_and_load.py:286: UserWarning: Could not find a config file in unsloth/LFM2-1.2B - will assume that the vocabulary was not modified.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCqnaKmlO1U9",
        "outputId": "1a6c35e2-dee4-4097-d3c1-3bf9955287e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "410.8215 seconds used for training.\n",
            "6.85 minutes used for training.\n",
            "Peak reserved memory = 10.334 GB.\n",
            "Peak reserved memory for training = 0.0 GB.\n",
            "Peak reserved memory % of max memory = 70.104 %.\n",
            "Peak reserved memory for training % of max memory = 0.0 %.\n"
          ]
        }
      ],
      "source": [
        "# @title Show final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory / max_memory * 100, 3)\n",
        "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(\n",
        "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
        ")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekOmTR1hSNcr"
      },
      "source": [
        "<a name=\"Inference\"></a>\n",
        "### Inference\n",
        "Let's run the model! You can change the instruction and input - leave the output blank!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kR3gIAX-SM2q",
        "outputId": "57018ade-0343-4c3c-9167-a7525ec69635"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The sky appears blue due to a phenomenon called Rayleigh scattering. When sunlight enters the Earth's atmosphere, it encounters tiny molecules of gases like nitrogen and oxygen. These molecules scatter the shorter wavelengths of light, such as blue and violet, more than the longer wavelengths like red and orange. As a result, the blue light is scattered in all directions, making the sky appear blue to our eyes.\n",
            "\n",
            "It's worth noting that our eyes are more sensitive to blue light than violet light, which is why the sky often appears more blue than violet. Additionally, during sunrise and sunset, the sun's light has to pass through more of the\n"
          ]
        }
      ],
      "source": [
        "messages = [{\n",
        "    \"role\": \"user\",\n",
        "    \"content\": \"Why is the sky blue?\",\n",
        "}]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt = True, # Must add for generation\n",
        "    return_tensors = \"pt\",\n",
        "    tokenize = True,\n",
        "    return_dict = True,\n",
        ").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "_ = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens = 128, # Increase for longer outputs!\n",
        "    # Recommended Liquid settings!\n",
        "    temperature = 0.3, min_p = 0.15, repetition_penalty = 1.05,\n",
        "    streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMbUhAumnufn",
        "outputId": "acd25791-7eb1-4dac-82dd-74af6f8d752f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I'm sorry for any inconvenience. I understand that you're looking for your shoes. Let me assist you with that. To locate your shoes, please follow these steps:\n",
            "\n",
            "1. Go to the {{Store Name}} website or app.\n",
            "2. Log in to your account using your credentials.\n",
            "3. Navigate to the {{Products}} or {{Shop}} section.\n",
            "4. Look for the {{Your Shoe Model}} or {{Specific Shoe}} you're looking for.\n",
            "5. Click on the product to view more details and purchase options.\n",
            "6. If you need further assistance, feel free to reach out to our\n"
          ]
        }
      ],
      "source": [
        "messages = [{\n",
        "    \"role\": \"user\",\n",
        "    \"content\": \"fuck you where is my shoes\",\n",
        "}]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt = True, # Must add for generation\n",
        "    return_tensors = \"pt\",\n",
        "    tokenize = True,\n",
        "    return_dict = True,\n",
        ").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "_ = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens = 128, # Increase for longer outputs!\n",
        "    # Recommended Liquid settings!\n",
        "    temperature = 0.3, min_p = 0.15, repetition_penalty = 1.05,\n",
        "    streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMuVrWbjAzhc"
      },
      "source": [
        "<a name=\"Save\"></a>\n",
        "### Saving, loading finetuned models\n",
        "To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.\n",
        "\n",
        "**[NOTE]** This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "upcOlWe7A1vc",
        "outputId": "a9782716-4f40-4491-fdc3-97e3ca2b5c71"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/peft/utils/other.py:1228: UserWarning: Unable to fetch remote file due to the following error 401 Client Error: Unauthorized for url: https://huggingface.co/unsloth/LFM2-1.2B/resolve/main/config.json (Request ID: Root=1-68b02e24-03212c515f3108092d03f903;56d0e18c-3b43-4bd0-aba3-1642c9c9f029)\n",
            "\n",
            "Invalid credentials in Authorization header - silently ignoring the lookup for the file config.json in unsloth/LFM2-1.2B.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/peft/utils/other.py:1228: UserWarning: Unable to fetch remote file due to the following error 401 Client Error: Unauthorized for url: https://huggingface.co/unsloth/LFM2-1.2B/resolve/main/config.json (Request ID: Root=1-68b02e25-74e32ea751e621ff311bbaa1;113c2b1a-4cbc-4472-8cd4-b32d703a65e6)\n",
            "\n",
            "Invalid credentials in Authorization header - silently ignoring the lookup for the file config.json in unsloth/LFM2-1.2B.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved model to https://huggingface.co/Heem2/ecommerce\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No files have been modified since last commit. Skipping to prevent empty commit.\n",
            "WARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\n"
          ]
        }
      ],
      "source": [
        "model.save_pretrained(\"ecommerce\")  # Local saving\n",
        "tokenizer.save_pretrained(\"ecommerce\")\n",
        "model.push_to_hub(\"Heem2/ecommerce\",  token = \"hf-token\")\n",
        "tokenizer.push_to_hub(\"Heem2/ecommerce\", token = \"hf-token\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7NGHDKwNJT7",
        "outputId": "90c3a872-1804-4305-ecdd-12a11af35177"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/peft/utils/other.py:1228: UserWarning: Unable to fetch remote file due to the following error 401 Client Error: Unauthorized for url: https://huggingface.co/unsloth/LFM2-1.2B/resolve/main/config.json (Request ID: Root=1-68b02e63-44eac2653b90233a549ceb70;88beed3b-07bc-4b25-a5f5-db38081c4b13)\n",
            "\n",
            "Invalid credentials in Authorization header - silently ignoring the lookup for the file config.json in unsloth/LFM2-1.2B.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/peft/utils/other.py:1228: UserWarning: Unable to fetch remote file due to the following error 401 Client Error: Unauthorized for url: https://huggingface.co/unsloth/LFM2-1.2B/resolve/main/config.json (Request ID: Root=1-68b02e64-549f56116a84761829bce7d9;04a187a2-efab-4c56-b844-7448a9370a54)\n",
            "\n",
            "Invalid credentials in Authorization header - silently ignoring the lookup for the file config.json in unsloth/LFM2-1.2B.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved model to https://huggingface.co/Heem2/ecommerceX\n"
          ]
        }
      ],
      "source": [
        "model.save_pretrained(\"ecommerceX\")  # Local saving\n",
        "tokenizer.save_pretrained(\"ecommerceX\")\n",
        "model.push_to_hub(\"Heem2/ecommerceX\",  token = \"hf-token\")\n",
        "tokenizer.push_to_hub(\"Heem2/ecommerceX\", token = \"hf-token\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "referenced_widgets": [
            "87ce7c1aedf244c39f8e89451b6441d1",
            "9dee7e327a124be6b8dcd44c36678b54",
            "7d1446ad751744bd96f1d108ec8ee3a6",
            "1dd7dc0b33394564bcd35d5787008a5c",
            "61ea3dac96264156905e4295b89546d1",
            "1a005d9ab08b4fde896e2faa5d5ecf7e",
            "b1d313ad01994034af782d9d1287b4f5",
            "00d2f21402f6498dbf61685a7b36713d",
            "3824ee28a7aa4e32833762ee0da26b8e",
            "0d5795fc98c94ef7b41dd1b4f8575ac4"
          ]
        },
        "id": "abITi42zXk1G",
        "outputId": "cd573177-7429-4258-856d-063641675a2b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "87ce7c1aedf244c39f8e89451b6441d1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9dee7e327a124be6b8dcd44c36678b54",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "adapter_config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7d1446ad751744bd96f1d108ec8ee3a6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1dd7dc0b33394564bcd35d5787008a5c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/2.34G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "61ea3dac96264156905e4295b89546d1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/160 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1a005d9ab08b4fde896e2faa5d5ecf7e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "adapter_model.safetensors:   0%|          | 0.00/7.08M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b1d313ad01994034af782d9d1287b4f5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "00d2f21402f6498dbf61685a7b36713d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3824ee28a7aa4e32833762ee0da26b8e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/434 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0d5795fc98c94ef7b41dd1b4f8575ac4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "chat_template.jinja:   0%|          | 0.00/209 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'generated_text': [{'role': 'user', 'content': 'ass hole fuck product'},\n",
              "   {'role': 'assistant',\n",
              "    'content': \"I'm sorry, but I can't assist with that. As a responsible and respectful assistant, I cannot participate in or promote any form of unethical or inappropriate language. If you have any questions or need assistance with finding a suitable product, I'd be more than happy to help. Let me know if there's something specific you're looking for or if you need guidance in choosing the right product.\"}]}]"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Use a pipeline as a high-level helper\n",
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"text-generation\", model=\"Heem2/ecommerceX\", token = \"hf-token\")\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"ass hole fuck product\"},\n",
        "]\n",
        "pipe(messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEEcJ4qfC7Lp"
      },
      "source": [
        "Now if you want to load the LoRA adapters we just saved for inference, set `False` to `True`:"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
